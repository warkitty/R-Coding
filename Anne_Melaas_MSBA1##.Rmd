---
title: "Final Exam"
author: "Anne Melaas"
subtitle: R for Data Science @ Hult
output:
  html_document:
    theme: readable
    highlight: pygments
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: yes
---

```{r chunk options, echo=FALSE}
# this chunk set up global chunk options. 
# You DO NOT need to run it. 
# Change NOTHING about it.
knitr::opts_chunk$set(warning=FALSE, message = FALSE) 


# The chunk below creates an written answer chunk for you to write answers. 
# You DO NOT need to run it. 
# Change NOTHING about it.
```

```{css question/answer styling, echo=FALSE}
/* do nothing to this chunk! */
.written_answer {
  padding: 1em;
  background: #FFF8DC;
  color: gray;
  border-left: 5px solid #fce68b;
  border-radius: 0px;
  margin-top: 2em;
  margin-bottom: 2em;
}
blockquote {
  font-size:16px;
  background-color: #f0f5ff;
}
```



# Directions {-}

There are 37 questions. Each question is tagged with **[QUESTION]**. Each question is worth 3 points. There are a total of 111 points. 

Enter your answers in the chunks provided. Do not create additional chunks. Do not code or type anywhere outside the chunks. 

There are two types of chunks. One for code, the other for writing. 

## Code chunks {-}

Code chunks look like this:

```{r, echo= TRUE, eval=FALSE}
# your code here
```

**Delete** `# your code here` and replace it with your code.

## Writing chunks {-}

The writing chunks look like this:

```{block, type = "written_answer"}
your answer here
```

You can write normal Markdown inside the writing chunks. 

**Delete** "your answer here" and replace it with your written answer. 

# Load the following packages {-}

Load the following packages:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
```

# Warm up

> **[QUESTION-1]** Calculate:

$$
\frac{10^2 + 4^3}{10^3 + 4^2}
$$

```{r }
(10^2 + 4^3) /(10^3 + 4^2)
```

> **[QUESTION-2]** Re-write the following code using pipes:

```{r}
mean(exp(sqrt(1:10)))
```

```{r}
exp(sqrt(1:10)) %>% 
  mean()
```

> **[QUESTION-3]** Draw 1000 random normal numbers with a mean of 3 and a standard deviation of 1. Then calculate the average of the 30th to 300th observations.

```{r}
set.seed(2020)
Q3.1 <- rnorm(n=1000, mean = 3, sd = 1)
mean(Q3.1[30:300])
```

> **[QUESTION-4]** Using `diamonds`, filter **out** (i.e., remove) diamonds whose cut is "Ideal", then plot the distribution of **log** prices **by** cut. Put each distribution in its own panel.

```{r}
diamonds%>%
  filter(cut != "Ideal")%>%
  group_by(cut)%>%
  mutate(log_price = log(price))%>%
  ggplot(.,mapping = aes(x= log_price))+
  geom_histogram()+
  facet_wrap(~cut)
  
```


# The value of (statistical) life

The Environmental Protection Agency (EPA) estimates the value of human life in US dollars. This allows the government to weight the costs and benefits of policies when human lives are at stake. 

While the EPA cannot place a value on each and every life, it can value **statistical** life -- the average life. 

The EPA estimate of an **unconditional** statistical life (i.e., not depending on factors like age, income and other characteristics) is $7.4 million. 

## Converting values

However, this estimate of $7.4 million is in 2006 dollars. We can convert this to dollars in another year with the following formula:

$$
X_b = X_a\frac{CPI_b}{CPI_a}
$$
where:

* $X_a$ is some dollar value in year a
* $X_b$ is the converted dollar value in year b
* $CPI_b$ is the consumer price index (CPI) in year b
* $CPI_a$ is the consumer price index (CPI) in year a

> **[QUESTION-5]** Write a function called `dollars_converter` that the implements the formula above. It should take three arguments:

* `value` for $X_a$
* `cpi_a` for $CPI_a$
* `cpi_b` for $CPI_b$

```{r}
dollars_converter <-  function(value, cpi_a,cpi_b){
output <- value * cpi_b / cpi_a
return(output)
} 
```

The CPI index in 2006 was 201.6. In 2018 it was 251.1. Use `dollars_converter` to convert the the value of a statistical life from 2006 dollars (7.4) to 2018 dollars.

```{r}
dollars_converter(value=7.4, cpi_a=201.6 , cpi_b= 251.1)
  
```


## Present value

Suppose a policy is proposed that will save **future** lives. How much are those lives worth right now? 

The **present value** of an asset (including a human life) is the asset's future value expressed in the present. The general formula is:

$$
PV = \frac{R_t}{(1 + d)^t}
$$

where:

* $R_t$ is the future value of some asset
* $d$ is the discount rate (a number between 0 and 1) -- the rate at which people discount benefits realized only in the future
* $t$ is time in years

> **[QUESTION-6]** Write a function called `present_value` that calculates present value of a statistical life $R$ as a function of time $t$ and the discount rate $d$. The function `present_value` should take three arguments:

* `r` for $R_t$ (defaulting to the value of a statistical life in 2018 dollars)
* `d` for $d$
* `t` for $t$

```{r}
present_value <- function(r = 9.216964, d, t) {
  output <- r/ ((1+d)^t)
  return(output)
}
```

> **[QUESTION-7]** Calculate the present value of a statistical life **five** years from now. Assume a discount rate of 5%. 

```{r}
present_value(d= 0.05, t=5)
```

> **[QUESTION-8]** Calculate the present value of a statistical life **fifty** years from now. Assume a discount rate of 5%. 

```{r}
present_value(d= 0.05, t=50)
```

> **[QUESTION-9]** Calculate the present value of a statistical life five years from now, but this time assume a discount rate of 10%. 

```{r}
present_value(d= 0.10, t=5)
```

> **[QUESTION-10]** Create a vector called `time` with the values one to fifty. Create an empty numerical vector called `pvs` the same length as `time`. Then write a loop that calculates the present value of a statistical for one to fifty years into the future. Assume a 5% discount rate. 

```{r}
time <- c(1:50)
pvs <- vector(mode="double", length = length(time))

for(i in seq_along(time)){
  pvs [i]<- present_value(d=0.05, t= time[i])
}
```

> **[QUESTION-11]** Create a tibble out of `time` and `pvs`. Then plot `pvs` over `time`. 

```{r}
tib <- tibble(time,pvs)

ggplot(data=tib, mapping=aes(x = time, y = pvs))+
  geom_line()
```

> **[QUESTION-12]** Write a function called `present_value_simulator` that calculates the present value of a statisitical life. The function will loop over a user-specified time interval and calculate the present value of a statistical life for a given discount rate. 

The function should have three arguments:

* `r``: the value of a statistical life (defaulting the value of a statistical life in 2018 dollars)
* `d`: the discount rate (a number between 0 and 1)
* `t_start`: the starting time period (an integer)
* `t_end`: the ending time period (an integer)


The function should return a tibble where the first column is the time vector and the second column are the present values. 

```{r}
present_value_simulator <- function(r =  9.216964, d, t_start, t_end) {
  time <- c(t_start : t_end)
  pvs <- vector(mode="double", length= length(time))
  
  for(i in seq_along(time)){
    pvs[i] <- r / ((1+d)^ time[i])
  }
  output <- tibble(time, pvs)
  return(output)
}

```


> **[QUESTION-13]** Use `present_value_simulator` to simulate the present value of a statistical life in 2018 dollars for `t_start = 0`, `t_end = 30`, and a discount rate of 10%. Pipe the output to `ggplot` and plot the present values over time.

```{r}
present_value_simulator(d = 0.1, t_start = 0, t_end = 30) %>% 
ggplot(., mapping = aes(x = time, y = pvs))+
  geom_line()
```

# COVID-19

Johns Hopkins collects and publishes data every day on COVID-19 (see [here](https://github.com/CSSEGISandData/COVID-19)).

Load the data from 11/30/20:

```{r load covid data, warning=FALSE, message=FALSE}
covid19 = read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/11-30-2020.csv")
```

> **[QUESTION-14]** Write code that finds the **five** countries with the highest number of confirmed cases as of 11/30/20.

```{r}
covid19 %>%
  group_by(Country_Region) %>% 
  summarize(sum_confirmed = sum(Confirmed)) %>% 
  slice_max(sum_confirmed, n=5)
```

> **[QUESTION-15]** Filter the data to keep only observations from the US and assign them to the object `covid_us`:

```{r}
covid_us <- covid19 %>%
  filter(Country_Region == "US")
```

*For the rest of this section use `covid_us`*. 

> **[QUESTION-16]** Which five states have the most cases?

```{r, message=FALSE}
covid_us%>%
  group_by(Province_State) %>% 
  summarize(sum_confirmed = sum(Confirmed)) %>% 
  slice_max(sum_confirmed, n=5)
```

> **[QUESTION-17]** Which five states have the most deaths?

```{r, message=FALSE}
covid_us%>%
  group_by(Province_State) %>% 
  summarize(dead = sum(Deaths)) %>% 
  slice_max(dead, n=5)
```

> **[QUESTION-18]** In the state with most cases, select the columns for state, county, confirmed cases and deaths, then find the county with the most cases:

```{r}

covid_us%>%
  filter(Province_State == "Texas") %>% 
  select(Province_State, Admin2, Confirmed, Deaths) %>% 
  slice_max(Confirmed, n=1)
```

> **[QUESTION-19]** Create a variable that calculates deaths over confirmed cases. Call this variable "cfr" (for "case fatality ratio"). You should get same results as "Case_Fatality_Ratio". **heads up!** 
There are some entries with either zero cases and non-zero deaths, and vice versa (this could be a data entry issue). Make sure you account for this when creating `cfr`. Use `ifelse` and give the value `NA` to entries for which you cannot calculate a case-fatality ratio.

```{r}
covid_us <- covid_us %>% 
  mutate(cfr = ifelse((Confirmed == 0 & Deaths != 0)|(Confirmed != 0 & Deaths == 0),NA, Deaths/Confirmed*100 ))
```

> **[QUESTION-20]** What are the five states or territories with the highest case-fatality ratios? 

```{r}
covid_us%>%
  group_by(Province_State) %>% 
  summarize(mean_cfr = mean(Case_Fatality_Ratio)) %>% 
  slice_max(mean_cfr,n=5)
```


> **[QUESTION-21]** Filter out the maximum value of `cfr`, then plot the cumulative distribution of `cfr` and use `geom_vline` to indicate what percent of the data is below a 5% mortality rate. Color the vertical line red. 

```{r}
cfr_max <- covid_us %>% 
  slice_max(cfr, n=1) %>% 
  select(cfr)

covid_us %>%  
  filter(cfr != cfr_max) %>% 
  ggplot(data=., aes(x=cfr))+
  stat_ecdf()+
  geom_vline(xintercept=5, color= "red")
```

# Boston Property

In this section you will explore data on property assessments in Boston. The data come from [Analyze Boston](https://data.boston.gov/). You can read more about the data [here](https://data.boston.gov/dataset/property-assessment/resource/bac18ae6-b8fd-4cd3-a61c-c5e1a11f716c)

## Data

Load all the data from FY18:

```{r load boston_property, message=FALSE, warning=FALSE}
boston_property = read_csv("https://query.data.world/s/pfqt62iufpcricbewykr3q6nrnpj6n")
```

The variable `AV_TOTAL` shows property assessments in US dollars.

> **[QUESTION-22]** The data are very large and messy so let's look at a sample. In the following chunk fill in the `filter` call to filter **out** (i.e., remove) observations with no property assessment or dwelling (i.e., where `AV_TOTAL`, `AV_LAND`, `LIVING_AREA` and `YR_BUILT` are equal to 0):

```{r}
set.seed(02132)
boston_property_sample <- boston_property %>% 
  filter( AV_TOTAL != 0 & AV_LAND != 0 & LIVING_AREA != 0 & YR_BUILT != 0) %>% # FILL THIS IN
  filter(R_TOTAL_RMS != 0)          %>% # remove "properties" with no rooms
  slice_sample(n=172841*0.8)              # draw an 80% sample
```

*For the next several questions make sure you are using `boston_property_sample`.*

> **[QUESTION-23]** Plot the distribution of property assessments (`AV_TOTAL`). 

```{r}
ggplot(data = boston_property_sample, aes(x= AV_TOTAL)) +
  geom_histogram()
```

> **[QUESTION-24]** Next, create a variable called `log_AV_TOTAL` that calculates the log of property prices and add it to the data.

```{r}
boston_property_sample <- boston_property_sample %>% 
  mutate(log_AV_TOTAL = log(AV_TOTAL))
```

> **[QUESTION-25]** Plot the distribution of **log** property assessments (`log_AV_TOTAL`). 

```{r}
ggplot(data = boston_property_sample, aes(x=log_AV_TOTAL))+
  geom_histogram()
```

> **[QUESTION-26]** Why bother calculating log property assessments? 

```{block, type = 'written_answer'}
Highlz skewed distribution of AV_TOTAL
```

> **[QUESTION-27]** Confirm that `log_AV_TOTAL` is normally distributed by showing that the mean and median are similar:

```{r}
mean(boston_property_sample$log_AV_TOTAL)
median(boston_property_sample$log_AV_TOTAL)
```

Let's focus the rest of our analysis on the zipcode with the most observations. 

> **[QUESTION-28]** Group observations by `ZIPCODE`, count the number of observations in each ZIPCODE, then identify the ZIPCODE with the most observations. 

```{r, message=FALSE}
boston_property_sample %>% 
  group_by(ZIPCODE) %>% 
  summarize(observation = n()) %>% 
  slice_max(observation, n=1)
```

> **[QUESTION-29]** Create a new data set called `boston_property_sample_ZIPCODE` but replace "ZIPCODE" with the actual zipcode (e.g., if the zipcode with the most observations was 02138, the data would be `boston_property_sample_02138`). Create this data set by filtering observations in that zipcode.

```{r}
boston_property_sample_02132 <- boston_property_sample %>% 
  filter(ZIPCODE == "02132")
```

## Summarizing the data

> **[QUESTION-30]** Calculate mean, median standard deviation and the count of log property assessments in `boston_property_sample_ZIPCODE`.

```{r}
boston_property_sample_02132%>%
  summarise(mean(log_AV_TOTAL),median(log_AV_TOTAL),sd(log_AV_TOTAL), n())
```

> **[QUESTION-31]** Make of a scatter plot of log prices on the y-axis and the number of bedrooms (`R_BDRMS`) on the x-axis. Include a regression line.

```{r}
ggplot(boston_property_sample_02132, mapping = aes(x=R_BDRMS, y=log_AV_TOTAL))+
  geom_point() +
  geom_smooth(method = "lm")
```

> **[QUESTION-32]** Interpret the plot.

```{block, type = 'written_answer'}
The more bedrooms the higher the log price. Residuals are symmetrically distributed around regression line for 0 to 7 bedrooms. 


```

## Modeling the data

> **[QUESTION-33]** Estimate a linear model of log property prices with `lm` and store the model in object `prices_model`. Model log prices as a function of:
 
* the total living area (`LIVING_AREA`)
* the number of bedrooms (`R_BDRMS`)
* the number of full bathrooms (`R_FULL_BTH`)
* the number of half bathrooms (`R_HALF_BTH`)
* the number of fireplaces (`R_FPLACE`)


```{r}
prices_model <- lm(formula = log_AV_TOTAL ~ LIVING_AREA + R_BDRMS + R_FULL_BTH + R_HALF_BTH + R_FPLACE, data= boston_property_sample_02132)
```

> **[QUESTION-34]** Use `summary` to view the results.

```{r}
summary(prices_model)
```

> **[QUESTION-35]** Interpret the coefficient on R_BDRMS **and** indicate whether the coefficient is statistically significant and why (or why not). Be sure to state the null and alternative hypotheses.

```{block, type = 'written_answer'}
H0: no relation between log price and number of bedrooms
HA: some relation between log price and number of bedrooms

coefficient indicates that everz additional bedroom the log price is increasng by 2.709e-03. p-value of 0.225 shows no statistical significance as it is higher than 0.05.

```

## Evaluating the model

**[Q]** Predict the **raw price** (i.e. in regular dollars, not log dollars) of a property with a living area of 2154 square feet, 4 bedrooms, 2 full bathrooms, 1 half bathroom and 1 fireplace. Hint:

$$
e^{log(x)} = x
$$

```{r}
# need to update this with latest coefficients
predicted_log_price = 1.243e+01 + 2.765e-04*2154 + 3.721e-03*4 + 4.193e-02*2 + 7.970e-02*1 + 6.478e-02*1
exp(predicted_log_price)
```

> **[QUESTION-36]** Use `broom::augment` to calculate predicted prices and the residuals. Then plot the distribution of residuals.

```{r}
broom::augment(prices_model) %>% 
  ggplot(data=., aes(x= .resid))+
  geom_histogram()
```

> **[QUESTION-37]** Interpret the plot.

```{block, type = 'written_answer'}
Residuals are symmetrical distributed around 0 -> good model


```



